<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="//nicolovaligi.com/theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="//nicolovaligi.com/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="//nicolovaligi.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Nicolò Valigi">
  <meta name="description" content="Posts and writings by Nicolò Valigi">

  <link href="https://nicolovaligi.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Nicolò Valigi Atom" />

<meta name="keywords" content="tensorflow, machine-learning">

  <title>
Naive Bayes classifiers in TensorFlow -
    Nicolò Valigi
  </title>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-67820015-1', 'auto');
  ga('send', 'pageview');

</script></head>

<body>
  <aside>
    <div id="user_meta">
      <a href="//nicolovaligi.com">
        <img src="/blog/images/logo_white.jpg" alt="logo">
      </a>
      <h2><a href="//nicolovaligi.com">Nicolò Valigi</a></h2>
      <p>Writing about Software, Robots, and Machine Learning.</p>
      <ul class="links">
        <li><a href="/pages/talks.html">Talks</a></li>
        <li><a href="/pages/robotics-for-developers-tutorial.html">Robotics for developers</a></li>
        <li><a href="/pages/research.html">Research</a></li>
        <li><a href="/feeds/all.atom.xml">RSS feed</a></li>
      </ul>

      <ul style="padding-left: 0;
                 margin-left: -5px;
                 list-style: none;
                 text-align: center;">

        <!-- github -->
        <li>
            <a href="https://github.com/nicolov">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

        <!-- mail -->
        <li>
            <a href="mailto:nicolo.valigi@gmail.com">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

        <!-- RSS -->
        <li>
            <a href="/feeds/all.atom.xml">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

    </ul>

    </div>
  </aside>

  <main>
    <header>
      <p>
      <a href="//nicolovaligi.com">Index</a> &nbsp; &brvbar; &nbsp;
      <a href="//nicolovaligi.com/tags.html">Tags</a> &nbsp; &brvbar; &nbsp;
      <a href="//nicolovaligi.com/archives.html">Archives</a>
      &brvbar; <a href="https://nicolovaligi.com/feeds/all.atom.xml">Atom</a>
      </p>
    </header>

<article>
  <div class="article_title">
    <h1><a href="//nicolovaligi.com/naive-bayes-tensorflow.html">Naive Bayes classifiers in TensorFlow</a></h1>
  </div>
  <div class="article_text">
    <p><img class="img-center" src="//nicolovaligi.com/tf_iris.png" style="max-width: 70%"/></p>
<p>Naive Bayes classifiers deserve their place in Machine Learning 101 as one of
the simplest and fastest algorithms for classification. This post explains a
very straightforward implementation in TensorFlow that I created as part of a
larger system. You can find the code <a href="https://github.com/nicolov/naive_bayes_tensorflow">here</a>.</p>
<p>The textbook application of Naive Bayes (NB) classifiers is spam filtering,
where word frequency counts are used to classify whether a given message is
spam or not. In this example, however, we're going to be using <em>continous</em>
data instead. More specifically, we'll be classifying flowers based on
measurements of their petals size.</p>
<h2>The algorithm</h2>
<p>Just a quick refresher on the NB algorithm: as with any classifier, the
training data is a set of <em>training examples</em> <span class="math">\(\textbf{x}\)</span>, each of which is
composed of <span class="math">\(n\)</span> features <span class="math">\(\textrm{x}_i = (x_{1}, x_{2}, ..., x_{n})\)</span> and their
corresponding class <span class="math">\(C_i\)</span> where <span class="math">\(i\)</span> is one of <span class="math">\(k\)</span> classes. The goal is to learn a conditional probability
model:</p>
<div class="math">$$ p(C_k | x_1, x_2, ..., x_k) $$</div>
<p>for each of the <span class="math">\(k\)</span> classes in the dataset. Later, we're going to see how a
simple rule can be used to make a decision on the basis of these conditional
probabilities. Intuitively, learning this multivariate distribution will
require a lot of data as the number of features grows. However, we can
simplify the task if we assume that <em>features are conditionally independent
given the class</em>. While this assumption never holds on real data, it results
in a single but surprisingly simple classifier.</p>
<p>The math develops as follows. First, we write down Bayes' theorem:</p>
<div class="math">$$ p(C_k | \mathbf{x}) = \frac{p_(C_k)p(\mathbf{x} | C_k)}{p(\mathbf{x})} $$</div>
<p>By definition of conditional probability, the numerator is just the <em>joint
probability distribution</em> <span class="math">\(p(C_k, \mathbf{x})\)</span>, and can be factored using the
chain rule:</p>
<div class="math">$$ p(C_k, \mathbf{x}) = p(x_1 | x_2, ..., x_n, C_k)p(x_2 | x_3, .., x_n, C_k)p(x_n | C_k)p(C_k) $$</div>
<p>By the <em>naive</em> assumption, the reciprocal conditioning among features can be
dropped. In other words, we assume that the value assumed by each feature depends
on the class only, and not on the values assumed by the other features. This
means that we can simplify the previous formula quite a bit:</p>
<div class="math">$$ p(C_k, \mathbf{x}) = p(x_1 | C_k)p(x_2 | C_k)...p(x_n | C_k)p(C_k) $$</div>
<p>Going back to Bayes' theorem, we observe that we can discard the denominator
since it's just a normalization factor that doesn't depend on the class. We
get:</p>
<div class="math">$$ p(C_k | \mathbf{x}) \sim p(C_k, \mathbf{x}) \sim p(c_K)p(x_1|C_k)p(x_2|C_k)p(x_3|C_k) \sim p(C_k)\prod_{i=1}^{n}p(x_i | C_k) $$</div>
<p>The last formula shows why NB is so convenient: all <span class="math">\(p(x_i | C_k)\)</span>
distributions can be learned independently. During training, we can split the
examples by label, then learn a univariate distribution for each of the
features given the class. For this example, we're going to assume that <span class="math">\(p(x_i | C_k)\)</span> are Gaussian distributions whose <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span> we can estimate easily
from the samples.</p>
<p>To wrap up the algorithm, we need to make a decision based on these conditional
probabilities. An intuitive and very common strategy is *<em>Maximum a Posteriori
(MAP)</em>: we simply pick the most likely class:</p>
<div class="math">$$ \underset{k \in \{1,..,K\}}{\arg\max}\ p(C_k) \prod_{i=1}^np(x_i | C_k) $$</div>
<h2>The scikit baseline</h2>
<p>With <code>sklearn</code>, loading the dataset and training the classifier is trivial:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">gnb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<p>This is how a plot of the 3 first principal components looks like in 3D:</p>
<p><img class="img-center" src="//nicolovaligi.com/iris_pca.png" style="max-width: 70%"/></p>
<h2>TensorFlow implementation</h2>
<p>You may want to follow along with the actual code <a href="https://github.com/nicolov/naive_bayes_tensorflow/blob/master/tf_iris.py">here</a>.</p>
<h3>Training</h3>
<p>We start by grouping the training samples based on their labeled class, and get
a <code>(nb_classes * nb_samples * nb_features)</code> array.</p>
<p>Based on the discussion above, we can fit individual Gaussian distributions to
each combination of labeled class and feature.  It's important to point out
that, even if we're feeding the data in one go, we are fitting a series of
<strong>univariate</strong> distributions, rather than a multivariate one:</p>
<div class="highlight"><pre><span></span><span class="n">mean</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">moments</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="n">points_by_class</span><span class="p">),</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
<p>In this trivial example, we're using <code>tf.constant</code> to get the
training data inside the TensorFlow graph. In real life, you probably want to
use <code>tf.placeholder</code> or even more performing alternatives like <code>tf.Data</code> (see
<a href="https://www.tensorflow.org/programmers_guide/datasets">the documentation</a>).</p>
<p>We take advantage of TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/tf/distributions"><code>tf.distributions</code>
module</a> to create
a Gaussian distribution with the estimated mean and variance:</p>
<div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">dist</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mean</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span><span class="p">))</span>
</pre></div>
<p>This distribution is the only thing we need to keep around for inference, and
it's luckily pretty compact, since the mean and variance are only <code>(nb_classes,
nb_features)</code>.</p>
<h3>Inference</h3>
<p>For inference, it's important to work in the <strong>log</strong> probability space to avoid
numerical errors due to repeated multiplication of small probabilities. We have:</p>
<div class="math">$$ \log p(C_k | \mathbf{x}) = \log p(C_k) + \sum_{i=1}^n \log p(\mathbf{x} | C_k) $$</div>
<p>To take care of the first term, we can assume that all classes are equally
likely (i.e. <strong>uniform</strong> prior):</p>
<div class="highlight"><pre><span></span><span class="n">priors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">nb_classes</span><span class="p">]</span> <span class="o">*</span> <span class="n">nb_classes</span><span class="p">))</span>
</pre></div>
<p>To compute the sum in the second term, we duplicate (<em>tile</em>) the feature vectors along a new "class" dimension, so that we can get probabilities from
the distribution in a single run:</p>
<div class="highlight"><pre><span></span><span class="c1"># (nb_samples, nb_classes, nb_features)</span>
<span class="n">all_log_probs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_classes</span><span class="p">]),</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">nb_classes</span><span class="p">,</span> <span class="n">nb_features</span><span class="p">]))</span>
</pre></div>
<p>The next step is to add up the contributions of each feature to the likelihood
of each class. In TensorFlow lingo, this is a <em>reduce</em> operation over the
features axis:</p>
<div class="highlight"><pre><span></span><span class="c1"># (nb_samples, nb_classes)</span>
<span class="n">cond_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">all_log_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
<p>We can then add up the priors and the conditional probabilities to get the
posterior distribution of the class label given the features:</p>
<div class="highlight"><pre><span></span><span class="n">joint_likelihood</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">priors</span><span class="p">,</span> <span class="n">cond_probs</span><span class="p">)</span>
</pre></div>
<p>In the derivation, we ignored the normalization factor, so the expression above
is not a proper probability distribution because it doesn't add up to 1. We
fix that by subtracting a normalization factor in log space using TensorFlow's
<a href="https://www.tensorflow.org/api_docs/python/tf/reduce_logsumexp"><code>reduce_logsumexp</code></a>.
Naively computing <code>log(sum(exp(..)))</code> here won't work here
because of numerical issues (see <a href="https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/">the logsumexp trick</a>).</p>
<div class="highlight"><pre><span></span><span class="n">norm_factor</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_logsumexp</span><span class="p">(</span>
    <span class="n">joint_likelihood</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">log_prob</span> <span class="o">=</span> <span class="n">joint_likelihood</span> <span class="o">-</span> <span class="n">norm_factor</span>
</pre></div>
<p>Finally, we exponentiate to get actual probabilities:</p>
<div class="highlight"><pre><span></span><span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span>
</pre></div>
<p>By feeding in a grid of points and drawing the contour lines at 0.5 probability, we get the nice plot at the top:</p>
<p><img class="img-center" src="//nicolovaligi.com/tf_iris.png" style="max-width: 70%"/></p>
<h2>Conclusion</h2>
<p>Building a simple Naive Bayes classifier in TensorFlow is a good learning
exercise to get familiar with TensorFlow's probability distributions and
practice the less common tensor operations.</p>
<h3>References</h3>
<ul>
<li><a href="http://kenzotakahashi.github.io/naive-bayes-from-scratch-in-python.html">Naive Bayes from scratch</a></li>
<li><a href="https://github.com/scikit-learn/scikit-learn/blob/f3320a6f/sklearn/naive_bayes.py">sklearn naive_bayes module</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="article_meta">
    <p>Posted on: dom 05 novembre 2017</p>
    <p>Category: <a href="//nicolovaligi.com/category/2017-11-05-naive-bayes-tensorflow.html">2017-11-05-naive-bayes-tensorflow</a>
 &ndash; Tags:
      <a href="//nicolovaligi.com/tag/tensorflow.html">tensorflow</a>,      <a href="//nicolovaligi.com/tag/machine-learning.html">machine-learning</a>    </p>
  </div>


</article>


    <div id="ending_message">
      <p>&copy; Nicolò Valigi. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme originally by Giulio Fidente on <a href="https://github.com/gfidente/pelican-svbhack" target="_blank">github</a>. </p>
    </div>
  </main>
</body>
</html>