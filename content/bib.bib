@article{Rublee2011,
author = {Rublee, Ethan and Bradski, Gary},
journal = {},
doi = {10.1109/ICCV.2011.6126544},
file = {:home/niko/Downloads/orb{\_}final.pdf:pdf},
isbn = {978-1-4577-1102-2},
issn = {1550-5499},
keywords = {Ethan Rublee Vincent Rabaud Kurt Konolige Gary Bra},
mendeley-groups = {DL {\&} SLAM},
pmid = {20033598},
title = {{ORB - an efficient alternative to SIFT or SURF}},
url = {http://www.willowgarage.com/sites/default/files/orb{\_}final.pdf},
year = {2011}
}

@ARTICLE{GalvezTRO12,
  author={G\'alvez-L\'opez, Dorian and Tard\'os, J. D.},
  journal={IEEE Transactions on Robotics},
  title={Bags of Binary Words for Fast Place Recognition in Image Sequences},
  year={2012},
  month={October},
  volume={28},
  number={5},
  pages={1188--1197},
  doi={10.1109/TRO.2012.2197158},
  ISSN={1552-3098}
}

@article{Jegou2010,
abstract = {We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms.},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia and P{\'{e}}rez, Patrick},
doi = {10.1109/CVPR.2010.5540039},
file = {:home/niko/Downloads/jegou{\_}compactimagerepresentation.pdf:pdf},
isbn = {9781424469840},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {DL {\&} SLAM},
pages = {3304--3311},
pmid = {22156101},
title = {{Aggregating local descriptors into a compact image representation}},
year = {2010}
}

@article{Zhou2014,
abstract = {Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks.},
author = {Zhou, Bolei and Lapedriza, Agata and Xiao, Jianxiong and Torralba, Antonio and Oliva, Aude},
file = {:home/niko/Downloads/places{\_}NIPS14.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
mendeley-groups = {DL {\&} SLAM},
pages = {487--495},
title = {{Learning Deep Features for Scene Recognition using Places Database}},
url = {http://papers.nips.cc/paper/5349-learning-deep-features-for-scene-recognition-using-places-database.pdf},
year = {2014}
}

@article{Fischer2014,
abstract = {Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching.},
archivePrefix = {arXiv},
arxivId = {1405.5769},
author = {Fischer, Philipp and Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1405.5769},
file = {:home/niko/Downloads/1405.5769v1.pdf:pdf},
journal = {arXiv},
mendeley-groups = {DL {\&} SLAM},
pages = {1--10},
title = {{Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT}},
url = {http://arxiv.org/abs/1405.5769},
year = {2014}
}

@article{Chen2013,
abstract = {Recently Convolutional Neural Networks (CNNs) have been shown to achieve state-of-the-art performance on various classification tasks. In this paper, we present for the first time a place recognition technique based on CNN models, by combining the powerful features learnt by CNNs with a spatial and sequential filter. Applying the system to a 70 km benchmark place recognition dataset we achieve a 75{\%} increase in recall at 100{\%} precision, significantly outperforming all previous state of the art techniques. We also conduct a comprehensive performance comparison of the utility of features from all 21 layers for place recognition, both for the benchmark dataset and for a second dataset with more significant viewpoint changes.},
archivePrefix = {arXiv},
arxivId = {1411.1509},
author = {Chen, Zetao and Lam, Obadiah and Jacobson, Adam and Milford, Michael},
eprint = {1411.1509},
file = {:home/niko/Downloads/1411.1509.pdf:pdf},
journal = {2014 Australasian Conference on Robotics and Automation (ACRA 2014)},
mendeley-groups = {DL {\&} SLAM},
pages = {8},
title = {{Convolutional Neural Network-based Place Recognition}},
year = {2013}
}

@article{Hou2015,
archivePrefix = {arXiv},
arxivId = {1504.05241},
author = {Hou, Yi and Zhang, Hong and Zhou, Shilin},
eprint = {1504.05241},
file = {:home/niko/Downloads/1504.05241.pdf:pdf},
isbn = {9781467391047},
mendeley-groups = {DL {\&} SLAM},
number = {August},
pages = {2238--2245},
title = {{Convolutional Neural Network-Based Image Representation for Visual Loop Closure Detection}},
year = {2015},
journal = {}
}

@article{Arandjelovic2015,
abstract = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture obtains a large improvement in performance over non-learnt image representations as well as significantly outperforms off-the-shelf CNN descriptors on two challenging place recognition benchmarks.},
archivePrefix = {arXiv},
arxivId = {1511.07247},
author = {Arandjelovi{\'{c}}, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef},
doi = {10.1109/CVPR.2016.572},
eprint = {1511.07247},
file = {:home/niko/Downloads/1511.07247v3.pdf:pdf},
journal = {Arxiv},
mendeley-groups = {DL {\&} SLAM},
title = {{NetVLAD: CNN architecture for weakly supervised place recognition}},
url = {http://arxiv.org/abs/1511.07247},
year = {2015}
}
