<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="//nicolovaligi.com/theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="//nicolovaligi.com/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="//nicolovaligi.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Nicolò Valigi">
  <meta name="description" content="Posts and writings by Nicolò Valigi">

  <link href="https://nicolovaligi.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Nicolò Valigi Atom" />

<meta name="keywords" content="tensorflow, machine-learning">

  <title>
Gradient Boosting in TensorFlow vs XGBoost -
    Nicolò Valigi
  </title>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-67820015-1', 'auto');
  ga('send', 'pageview');

</script></head>

<body>
  <aside>
    <div id="user_meta">
      <a href="//nicolovaligi.com">
        <img src="/blog/images/logo_white.jpg" alt="logo">
      </a>
      <h2><a href="//nicolovaligi.com">Nicolò Valigi</a></h2>
      <p>Writing about Software, Robots, and Machine Learning.</p>
      <ul class="links">
        <li><a href="/pages/talks.html">Talks</a></li>
        <li><a href="/pages/robotics-for-developers-tutorial.html">Robotics for developers</a></li>
        <li><a href="/pages/research.html">Research</a></li>
        <li><a href="/feeds/all.atom.xml">RSS feed</a></li>
      </ul>

      <ul style="padding-left: 0;
                 margin-left: -5px;
                 list-style: none;
                 text-align: center;">

        <!-- github -->
        <li>
            <a href="https://github.com/nicolov">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

        <!-- mail -->
        <li>
            <a href="mailto:nicolo.valigi@gmail.com">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

        <!-- RSS -->
        <li>
            <a href="/feeds/all.atom.xml">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

    </ul>

    </div>
  </aside>

  <main>
    <header>
      <p>
      <a href="//nicolovaligi.com">Index</a> &nbsp; &brvbar; &nbsp;
      <a href="//nicolovaligi.com/tags.html">Tags</a> &nbsp; &brvbar; &nbsp;
      <a href="//nicolovaligi.com/archives.html">Archives</a>
      &brvbar; <a href="https://nicolovaligi.com/feeds/all.atom.xml">Atom</a>
      </p>
    </header>

<article>
  <div class="article_title">
    <h1><a href="//nicolovaligi.com/gradient-boosting-tensorflow-xgboost.html">Gradient Boosting in TensorFlow vs XGBoost</a></h1>
  </div>
  <div class="article_text">
    <p>Tensorflow 1.4 was released a few weeks ago with an implementation of Gradient
Boosting, called <strong>TensorFlow Boosted Trees (TFBT)</strong>. Unfortunately, the
<a href="https://arxiv.org/abs/1710.11555">paper</a> does not have any benchmarks, so I
ran some against XGBoost.</p>
<p>For many Kaggle-style data mining problems, XGBoost has been the go-to solution since its release in 2006. It's probably as close to an out-of-the-box machine learning algorithm as you can get today, as it gracefully handles un-normalized or missing data, while being accurate and fast to train.</p>
<p>The code to reproduce the results in this article is <a href="https://github.com/nicolov/gradient_boosting_tensorflow_xgboost">on GitHub</a>.</p>
<h2>The experiment</h2>
<p>I wanted a decently sized dataset to test the scalability of the two solutions, so I picked the <strong>airlines dataset</strong> available <a href="http://stat-computing.org/dataexpo/2009/">here</a>. It has around 120 million data points for all commercial flights
within the USA from 1987 to 2008. The features include origin and destination
airports, date and time of departure, arline, and flight distance. I set up a
straightforward binary classification task that tries to predict whether a flight
would be more than 15 minutes late.</p>
<p>I sampled 100k flights from 2006 for the training set, and 100k flights from
2007 for the test set. Sadly, roughly 20% of flights were more than 15 minutes
late, a fact that doesn't reflect well on the airline industry :D. It's easy
to see how strongly departure time throughout the day correlates with the
likelihood of delay:</p>
<p><img class="img-center" src="//nicolovaligi.com/by_hour.png" style="max-width: 80%"/></p>
<p>I did not do any feature engineering, so the list of features is very basic:</p>
<div class="highlight"><pre><span></span>Month
DayOfWeek
Distance
CRSDepTime
UniqueCarrier
Origin
Dest
</pre></div>
<p>I used the scikit-style wrapper for XGBoost, which makes training and
prediction from NumPy arrays a two-line affair (<a href="https://github.com/nicolov/gradient_boosting_tensorflow_xgboost/blob/master/do_xgboost.py">code</a>). For TensorFlow, I used
<code>tf.Experiment</code>, <code>tf.learn.runner</code>, and the NumPy input functions to save some
boilerplate (<a href="https://github.com/nicolov/gradient_boosting_tensorflow_xgboost/blob/master/do_tensorflow.py">code</a>). TODO</p>
<h2>Results</h2>
<p>I started out with XGBoost and a decent guess at the hyperparameters, and
immediately got an <a href="https://stats.stackexchange.com/questions/132777/what-does-auc-stand-
for-and-what-is-it">AUC
score</a> I was happy with. When I tried the same settings on
TensorFlow Boosted Trees, I didn't even have enough patience for the training
to end!</p>
<p>While I kept <code>num_trees=50</code> and <code>learning_rate=0.1</code> for both models, I ended
up having to tweak the TF Boosted Trees <code>examples_per_layer</code> knob using an
hold-out set. It's likely that this is related to the novel <em>layer-by-layer</em>
learning algorithm featured in the TFBT paper, but I haven't dug in deeper. As
a starting point for comparison, I selected two values (1k and 5k) that
yielded similar training times and accuracy to XGBoost. Here's how the results
look like:</p>
<p><img class="img-center" src="//nicolovaligi.com/roc.png" style="max-width: 80%"/></p>
<hr>
<p><img class="img-center" src="//nicolovaligi.com/execution_time.png" style="max-width: 80%"/></p>
<hr>
<p>Accuracy numbers:</p>
<div class="highlight"><pre><span></span>                   Model  AUC score
-----------------------------------
                 XGBoost       67.6
TensorFlow (1k ex/layer)       62.1
TensorFlow (5k ex/layer)       66.1
</pre></div>
<hr>
<p>Training runtime:</p>
<div class="highlight"><pre><span></span>./do_xgboost.py --num_trees=50
42.06s user 1.82s system 1727% cpu 2.540 total

./do_tensorflow.py --num_trees=50 --examples_per_layer=1000
124.12s user 27.50s system 374% cpu 40.456 total

./do_tensorflow.py --num_trees=50 --examples_per_layer=5000
659.74s user 188.80s system 356% cpu 3:58.30 total
</pre></div>
<hr>
<p>Neither of the two settings shown for TensorFlow could match the training
time/accuracy of XGBoost. Besides the disadvantage in <code>user</code> time (total CPU
time used), it also seems that TensorFlow isn't very effective at
parallelizing on multiple cores either, leading to a massive gap in <code>total</code>
(i.e. wall) time too. XGBoost has no trouble loading 16 of the 32 cores in my
box (and can do better when using more trees), whereas TensorFlow uses less
than 4. I guess that the whole "distributed TF" toolbox could be used to make
TFBT scale better, but that seems overkill just to make full use of a <em>single</em>
server.</p>
<h2>Conclusion</h2>
<p>With a few hours of tweaking, I couldn't get TensorFlow's Boosted Trees
implementation to match XGBoost's results, neither in training time nor
accuracy. This immediately disqualifies it from many of the quick-n-dirty
projects I would use XGBoost for. The limited parallelism of the implementation
also means that it wouldn't scale up to big datasets either.</p>
<p>TensorFlow Boosted Trees might make sense within an infrastructure that's
heavily invested in TensorFlow tooling already. TensorBoard and the data
loading pipeline are two features that work fine on Boosted Trees too and
could easily migrated from other Deep Learning projects based on TensorFlow.
However, TF Boosted Trees won't be very useful in most cases until the
implementation can match the performance of XGBoost (or I learn how to tune
it).</p>
<p>To reproduce my results, get the <a href="https://github.com/nicolov/gradient_boosting_tensorflow_xgboost">training code on GitHub</a>.</p></hr></hr></hr></hr>
  </div>
  <div class="article_meta">
    <p>Posted on: dom 26 novembre 2017</p>
    <p>Category: <a href="//nicolovaligi.com/category/2017-11-26-gradient-boosting-tensorflow-xgboost.html">2017-11-26-gradient-boosting-tensorflow-xgboost</a>
 &ndash; Tags:
      <a href="//nicolovaligi.com/tag/tensorflow.html">tensorflow</a>,      <a href="//nicolovaligi.com/tag/machine-learning.html">machine-learning</a>    </p>
  </div>


</article>


    <div id="ending_message">
      <p>&copy; Nicolò Valigi. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme originally by Giulio Fidente on <a href="https://github.com/gfidente/pelican-svbhack" target="_blank">github</a>. </p>
    </div>
  </main>
</body>
</html>