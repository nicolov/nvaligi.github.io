<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="http://nicolovaligi.com/theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="http://nicolovaligi.com/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="http://nicolovaligi.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Nicolò Valigi">
  <meta name="description" content="Posts and writings by Nicolò Valigi">


<meta name="keywords" content="robotics, perception">

  <title>
Robotics for developers 3/6: using the camera -
    Nicolò Valigi
  </title>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-67820015-1', 'auto');
  ga('send', 'pageview');

</script></head>

<body>
  <aside>
    <div id="user_meta">
      <a href="http://nicolovaligi.com">
        <img src="/blog/images/logo_white.jpg" alt="logo">
      </a>
      <h2><a href="http://nicolovaligi.com">Nicolò Valigi</a></h2>
      <p>Chronicles of learning.</p>
      <ul class="links">
        <li><a href="http://nicolovaligi.com/pages/tutorial-on-robotics-for-developers.html">Tutorial on robotics for developers</a></li>
      </ul>

      <ul style="padding-left: 0;
                 margin-left: -5px;
                 list-style: none;
                 text-align: center;">
        <li>
            <a href="https://github.com/nicolov">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>
        <li>
            <a href="mailto:nicolo.valigi@gmail.com">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>
    </ul>

    </div>
  </aside>

  <main>
    <header>
      <p>
      <a href="http://nicolovaligi.com">Index</a> &nbsp; &brvbar; &nbsp;
      <a href="http://nicolovaligi.com/tags.html">Tags</a> &nbsp; &brvbar; &nbsp;
      <a href="http://nicolovaligi.com/archives.html">Archives</a>
      </p>
    </header>

<article>
  <div class="article_title">
    <h1><a href="http://nicolovaligi.com/robotics-for-developers/using-the-camera.html">Robotics for developers 3/6: using the camera</a></h1>
  </div>
  <div class="article_text">
    <p>The previous post showed how to use probability theory and non-linear optimization to integrate different sources of information. But what kind of data does the observation of a fiducial marker actually produce? How can we find out the position of the camera with respect to the marker once we know its size and position within the image? In other words, what's the form of the measurement factor that gets added to the factor graph?</p>
<h3>The pinhole model</h3>
<p>We're going to use concepts from <em>geometric computer vision</em> to exploit the relationship between the position of the marker corners in the captured image and in the real world. To do this, we need to introduce a model for the physical behavior of a digital camera. While things can generally get hairy here, it's enough to say that a camera <em>projects</em> 3D points in the environment to 2D points in the sensor plane. <sup id="fnref:projection"><a class="footnote-ref" href="#fn:projection" rel="footnote">1</a></sup> Some basic geometry helps here, as shown in the following image (the dotted frame is the sensor plane):</p>
<p><img )="" class="img-center" src="http://nicolovaligi.com/robotics-for-developers/pinhole_model.png" style="max-width: 350px"/></p>
<p>The picture represents the relationship between world point <span class="math">\(P\)</span> (<code>world_p</code> 3-vector), and image point <span class="math">\(p\)</span>, (<code>image_p</code> 2-vector). There's a straightforward mathematical relationship between <span class="math">\(P\)</span> and <span class="math">\(p\)</span>, that can be expressed as: <sup id="fnref:notation"><a class="footnote-ref" href="#fn:notation" rel="footnote">2</a></sup></p>
<div class="highlight"><pre><span></span>image_p[0] = world_p[0] * f_x / world_p[2] + o_x
image_p[1] = world_p[1] * f_y / world_p[2] + o_y
</pre></div>
<p>As you will have noticed, there are three unknown parameters in the equation: <code>f_x</code>, <code>f_y</code>, <code>o_x</code>, <code>o_y</code>. These numbers are <em>intrinsic</em> properties of the imaging system (camera + lens combination) and are the same for any point in any frame. They're usually obtained through the process of <em>intrinsic calibration</em> of the camera, which we're going to happily skip, since it's well-covered online and doesn't require much insight. There's a bit more on this topic, namely introducing corrections for lens imperfections, but it's not very important for a full understanding.</p>
<p>In mathematical terms, the pinhole projection equations can be written as:</p>
<div class="math">$$
\begin{cases}
x = \frac{X\ f_x}{Z} + o_x \\
y = \frac{Y\ f_y}{Z} + o_y
\end{cases}
$$</div>
<h3>Reprojection error</h3>
<p>Thanks to the previous two sections, we have all the building blocks needed to write down the <strong>measurement model</strong> for a single point. Given a measurement (in our case, the position of a corner in a captured image), and the current estimate of the camera position (the <em>state</em>), the measurement model will tell the optimizer:</p>
<ul>
<li>how well the current estimated state agrees with the measurement,</li>
<li>the direction in which to "wiggle" the estimated state to improve such agreement <sup id="fnref:jacobian"><a class="footnote-ref" href="#fn:jacobian" rel="footnote">3</a></sup></li>
</ul>
<p>To make things simpler, we can assume that the camera is at position <span class="math">\((0, 0, 0)\)</span>. Later, we'll see how it's generally more convenient to have one of the markers in this position, and leave the camera floating in space.</p>
<p>In any case, given that we know the position of the camera, the situation is just like the projection picture above, and the state will simply be the position of the corner point <em>in the world</em>.</p>
<p>The pinhole equations, expressed here by the function <code>project</code>, are all we need to write the measurement model: <sup id="fnref:meas_model"><a class="footnote-ref" href="#fn:meas_model" rel="footnote">4</a></sup></p>
<div class="highlight"><pre><span></span>error[0] = measured_image_p[0] - project(world_p)[0]
error[1] = measured_image_p[1] - project(world_p)[1]
</pre></div>
<p>The equations above compute the <strong>reprojection error</strong> for a single point
correspondence. The array notation emphasizes that the reprojection error is a
two dimensional vector (one horizontal and one vertical term). Note how the vector dimensions all match: <code>project</code> takes a 3D points and returns a 2D point, just like <code>measured_image_p</code> is.</p>
<p>In our case, we need to consider multiple points at the same time (at least 4). There's nothing preventing us from extending the <code>error</code> vector above with additional point correspondences. However, the next section will explain why this is a bad idea.</p>
<h3>What about the other corners?</h3>
<p>The problem with having a 8-vector error for each marker (4 corners * 2 points per corner) is that each corner would be optimized independently. This is not correct though since the position of each is related to the others (as they're all part of the same rigid object - the marker).</p>
<p>The way we solve this is by adopting a better definition of the state. Instead of having the position of <em>points</em>, we're going to optimize over the <em>marker pose</em>. Note that I've wrote <em>pose</em>, not position. Since the marker is a whole object, not just a single point, we need to define its <em>orientation</em> as well as its position.</p>
<p>While the basic concept is straightforward (define a set of rotations to align the marker with some reference, such as a floor tile), rotation representations are a dime a dozen and wildly confusing. Luckily, we're going to take advantage of classes and functions offered by the GTSAM framework to tackle most of this issues. <sup id="fnref:rot_repr"><a class="footnote-ref" href="#fn:rot_repr" rel="footnote">5</a></sup></p>
<p>Instead, let's focus on using this new state to write the right reprojection error for a whole marker. We can look at the marker pose as the position of the center point of the marker, together with the orientation of its frame of reference. To compute the position of each of the 4 corner points, we just need to <em>compose</em> the marker pose with vectors going from the center of the marker (our reference point) to each of them. Luckily, these vectors are easy to compute, since we know the side length of the marker (we just need to pay some attention to the signs and directions). The figure below shows a 2D simplification of the concept:</p>
<p><a href="http://nicolovaligi.com/robotics-for-developers/3_camera/marker_comp.pdf"><img class="img-center" src="http://nicolovaligi.com/__pdf_previews__/robotics-for-developers/3_camera/marker_comp.pdf.png" style="max-width: 300px"/></a></p>
<p>While there's <em>much</em> more mathematics behind this, we can use the power of software abstraction and ignore most of it. Indeed, most optimization frameworks make it easy to abstract away rotations and translations without caring about the underlying representation.</p>
<h2>Leveraging an existing marker detector and datasets</h2>
<p>As mentioned in the previous post, we're going to be taking advantage of
datasets provided by the community instead of setting up our own hardware
platform. This also means that we're going to have to choose the same marker
detector they did.</p>
<p>For this project, I've picked datasets from the <a href="https://bitbucket.org/adrlab/rcars/overview">RCARS
project</a> by ETH Zurich, since they
are recorded with a nice stereo camera and include ground truth and
accelerometer data that will be useful as we progress in the project. For reference, that's how the images look like:</p>
<p><img class="img-center" src="http://nicolovaligi.com/robotics-for-developers/frame_example.jpg" style="width: 80%"/></p>
<p>Since the marker detector they used doesn't compile on Ubuntu 16.04, I've decided
to cheat, ran the detection on a different machine, and recorded the results in
a new file. This also makes for a smaller download, since we won't need the
images anymore, but just the position of the corners and their ids. <sup id="fnref:rosbag"><a class="footnote-ref" href="#fn:rosbag" rel="footnote">6</a></sup></p>
<p>Let's take one of these datasets and examine it with <code>rosbag info</code>:</p>
<div class="highlight"><pre><span></span><span class="n">types</span><span class="o">:</span>       <span class="n">geometry_msgs</span><span class="o">/</span><span class="n">TransformStamped</span> <span class="o">[</span><span class="n">b5764a33bfeb3588febc2682852579b0</span><span class="o">]</span>
             <span class="n">rcars_detector</span><span class="o">/</span><span class="n">TagArray</span>        <span class="o">[</span><span class="n">f8c7f4812d2c3fcc55ab560a8de1d680</span><span class="o">]</span>
             <span class="n">sensor_msgs</span><span class="o">/</span><span class="n">CameraInfo</span>         <span class="o">[</span><span class="n">c9a58c1b0b154e0e6da7578cb991d214</span><span class="o">]</span>
             <span class="n">sensor_msgs</span><span class="o">/</span><span class="n">Imu</span>                <span class="o">[</span><span class="mi">6</span><span class="n">a62c6daae103f4ff57a132d6f95cec2</span><span class="o">]</span>
<span class="n">topics</span><span class="o">:</span>      <span class="sr">/cam0/camera_info       1269 msgs    : sensor_msgs/</span><span class="n">CameraInfo</span>        
             <span class="sr">/imu0                  12690 msgs    : sensor_msgs/</span><span class="n">Imu</span>               
             <span class="sr">/rcars/detector/tags    1269 msgs    : rcars_detector/</span><span class="n">TagArray</span>       
             <span class="sr">/vicon/auk/auk          6343 msgs    : geometry_msgs/</span><span class="n">TransformStamped</span>
</pre></div>
<p>For now, all the data we need to feed into the optimizer is in the <code>/rcars/detector/tags</code> and <code>/cam0/camera_info</code> (that contains the intrinsics parameters as described above).</p>
<h2>Putting everything together - code!</h2>
<p>What follows is a walk-through of the code I've pushed <a href="https://github.com/nicolov/robotics_for_developers">here</a>. We're going to
gloss over most of the boring sections and focus on the juicy parts, as it's
easy to pick up the former from examples and ROS tutorials. <sup id="fnref:gtsam_examples"><a class="footnote-ref" href="#fn:gtsam_examples" rel="footnote">7</a></sup></p>
<p>To start, we setup containers for the problem structure and the values associated with each element of the state:</p>
<div class="highlight"><pre><span></span><span class="n">gtsam</span><span class="o">::</span><span class="n">NonlinearFactorGraph</span> <span class="n">graph</span><span class="p">;</span>
<span class="n">gtsam</span><span class="o">::</span><span class="n">Values</span> <span class="n">estimate</span><span class="p">;</span>
</pre></div>
<p>Each piece of the state needs a label (<code>gtsam::Symbol</code>) that will identify it during the optimization problem. We create one for the camera pose and one for the marker pose:</p>
<div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">s_camera</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">Symbol</span><span class="p">(</span><span class="sc">'C'</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="k">auto</span> <span class="n">s_marker</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">Symbol</span><span class="p">(</span><span class="sc">'M'</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</pre></div>
<p>The optimization needs to start somewhere, so we pick initial guesses and insert them in the <code>Values</code> container:</p>
<div class="highlight"><pre><span></span><span class="n">estimate</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">s_marker</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">Pose3</span><span class="p">());</span>
<span class="k">auto</span> <span class="n">cam_guess</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">Pose3</span><span class="p">(</span><span class="n">gtsam</span><span class="o">::</span><span class="n">Rot3</span><span class="p">(),</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">Point3</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">));</span>
<span class="n">estimate</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="n">s_camera</span><span class="p">,</span> <span class="n">cam_guess</span><span class="p">);</span>
</pre></div>
<p>In this example, the initial guess for the camera pose is 0.5 meters away from
the marker, in the <span class="math">\(z\)</span> direction. As with any nonlinear optimization problem,
the initial guesses should be chosen quite carefully, otherwise the optimizer
may fail. This has been the subject of many a PhD's theses, so we won't cover
it here.</p>
<p>The next step is picking a reasonable <em>noise model</em> to encode our degree of trust
on the measurements. We're going to be using Gaussian distributions here since
the marker detection is pretty robust and unlikely to produce <strong>outliers</strong>
(hopelessly wrong values, as opposed to slightly incorrect measurements due to
normal errors). Robust handling of outliers is another can of worms which is
usually taken care of using <em>robust estimators</em>.</p>
<div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">pixel_noise</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">noiseModel</span><span class="o">::</span><span class="n">Isotropic</span><span class="o">::</span><span class="n">shared_ptr</span><span class="p">(</span>
    <span class="n">gtsam</span><span class="o">::</span><span class="n">noiseModel</span><span class="o">::</span><span class="n">Isotropic</span><span class="o">::</span><span class="n">Sigma</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">));</span>
</pre></div>
<p>The standard deviation of 1 pixel is pretty common in practice. It's now time to create a <strong>prior factor</strong> to encode our belief that the marker pose is close to the identity. The way this works is that I've decided that I want the marker to be in the origin pose, and thus create the prior accordingly. I could have set this prior anywhere (for example, 5 meters east), with no changes. However, a prior is indeed required otherwise the information in the measurement would not be enough to constrain the camera pose. This is somewhat intuitive, since it's clear that observing an object at a certain distance doesn't tell us where the camera is at all. That's why we need to add this additional belief.</p>
<div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">origin_noise</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">noiseModel</span><span class="o">::</span><span class="n">Isotropic</span><span class="o">::</span><span class="n">shared_ptr</span><span class="p">(</span>
    <span class="n">gtsam</span><span class="o">::</span><span class="n">noiseModel</span><span class="o">::</span><span class="n">Isotropic</span><span class="o">::</span><span class="n">Sigma</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">));</span>

<span class="n">graph</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span>
  <span class="n">gtsam</span><span class="o">::</span><span class="n">PriorFactor</span><span class="o">&lt;</span><span class="n">gtsam</span><span class="o">::</span><span class="n">Pose3</span><span class="o">&gt;</span><span class="p">(</span><span class="n">s_marker</span><span class="p">,</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">Pose3</span><span class="p">(),</span> <span class="n">origin_noise</span><span class="p">));</span>
</pre></div>
<p>It's finally time to add the measurement corresponding to the marker observation. We do this with the <code>MarkerFactor</code> class that embeds the reprojection error calculation:</p>
<div class="highlight"><pre><span></span><span class="n">graph</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span>
  <span class="n">MarkerFactor</span><span class="p">(</span><span class="n">corners</span><span class="p">,</span> <span class="n">pixel_noise</span><span class="p">,</span> <span class="n">s_camera</span><span class="p">,</span> <span class="n">s_marker</span><span class="p">,</span> <span class="n">K_</span><span class="p">,</span> <span class="n">tag_size_</span><span class="p">));</span>
</pre></div>
<p>This factor is more complicated than the others, since it must be connected to both the marker and the camera poses and also needs the camera calibration (<code>K_</code>) and the side length of the marker (<code>tag_size_</code>).</p>
<p>Now that the factor graph is set up, we can kick off the optimization and relax:</p>
<div class="highlight"><pre><span></span><span class="k">auto</span> <span class="n">result</span> <span class="o">=</span> <span class="n">gtsam</span><span class="o">::</span><span class="n">GaussNewtonOptimizer</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">estimate</span><span class="p">).</span><span class="n">optimize</span><span class="p">();</span>
<span class="k">auto</span> <span class="n">opt_camera_pose</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">at</span><span class="o">&lt;</span><span class="n">gtsam</span><span class="o">::</span><span class="n">Pose3</span><span class="o">&gt;</span><span class="p">(</span><span class="n">s_camera</span><span class="p">);</span>
</pre></div>
<p>The choice of the optimization algorithm has a significant effect on the
convergence properties, speed and accuracy of the solution. Since the problem
here is pretty straightforward, a basic textbook choice like Gauss-Newton's is
fine.</p>
<h2>Comparison with OpenCV</h2>
<p>Our current approach is definitely overkill for such a simple measurement, and indeed OpenCV provides a built-in function to replace it all. Indeed, the <code>solvePnP</code> function will solve the 2D-&gt;3D scene reconstruction problem in the exact same way as we did.</p>
<p>We compare its results with our solution by plotting one coordinate of the camera position over time:</p>
<div>
<a href="https://plot.ly/~nikoperugia/11/" style="display: block; text-align: center;" target="_blank" title="Camera Z position [m] vs Time [s]"><img alt="Camera Z position [m] vs Time [s]" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" src="https://plot.ly/~nikoperugia/11.png" style="max-width: 100%;width: 600px;" width="600"/></a>
<script async="" data-plotly="nikoperugia:11" src="https://plot.ly/embed.js"></script>
</div>
<p>The numbers are so close that you actually need to hover to see both. You can also see a brief hole in the plot, corresponding to a few moments in which the markers was outside the frame.</p>
<h2>Next steps</h2>
<p>Now that we've set up the plumbing and checked that our computer vision fundamentals are sound, we can quickly iterate to make the project more useful, starting with tracking the robot as its camera moves over time. This is the subject of the next post in the series.</p>
<h3>Notes</h3>
<div class="footnote">
<hr/>
<ol>
<li id="fn:projection">
<p>The reference book for this branch of computer vision is <a href="https://www.amazon.com/Multiple-View-Geometry-Computer-Vision/dp/0521540518?ie=UTF8&amp;n=507846&amp;qid=1126195435&amp;redirect=true&amp;ref_=pd_bbs_1&amp;s=books&amp;sr=8-1&amp;v=glance">Hartley's</a>. <a class="footnote-backref" href="#fnref:projection" rev="footnote" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
<li id="fn:notation">
<p>I'll try to keep mathematical notation to a minimum throughout, and use some sort of pseudocode instead. Whether it's <em>numpy</em>, <em>matlab</em>, or <em>Eigen</em>, most code written in the real world handles vectors and matrices nicely. Explicit for loop are rarely needed for mathematics. <a class="footnote-backref" href="#fnref:notation" rev="footnote" title="Jump back to footnote 2 in the text">↩</a></p>
</li>
<li id="fn:jacobian">
<p>Mathematically, this information is carried by the <strong>Jacobian matrix</strong>. GTSAM's reference noted in the previous post has plenty of information about its usage, its calculation, and its hair-pulling properties. Thanks to the <strong>chain rule</strong>, however, it's easy to build the Jacobian of a complex computation as long as we know how to compute the Jacobian of each of its parts. The concept is similar to <em>backpropagation</em> is neural networks. <a class="footnote-backref" href="#fnref:jacobian" rev="footnote" title="Jump back to footnote 3 in the text">↩</a></p>
</li>
<li id="fn:meas_model">
<p>The GTSAM tutorial has more in-depth information about writing a custom measurement factor. You should also refer at the code in <code>MarkerFactor.h</code> to see how I implemented this particular measurement. By chaining together functions (and keeping track of their Jacobian), writing basic factors is usually painless. <a class="footnote-backref" href="#fnref:meas_model" rev="footnote" title="Jump back to footnote 4 in the text">↩</a></p>
</li>
<li id="fn:rot_repr">
<p>The most common representations for rotations are <em>rotation matrices</em>, <em>Euler angles</em>, and <em>quaternions</em>. Each of these has plus and cons depending on the objective and the situation. In addition, GTSAM also uses ideas from Lie algebras for certain kinds of operations (more information <a href="https://bitbucket.org/gtborg/gtsam/raw/25bf277cde12211d1c553b3fc6e59b3f942c79de/doc/LieGroups.pdf">here</a>). Another good introduction to optimization in the rotation space is <a href="http://arxiv.org/abs/1107.1119">here</a>. <a class="footnote-backref" href="#fnref:rot_repr" rev="footnote" title="Jump back to footnote 5 in the text">↩</a></p>
</li>
<li id="fn:rosbag">
<p>In the ROS world, datasets are usually recorded using the <code>rosbag</code> tool, which helps researches store data during experiments and play it back to test algorithms. If you want to work with ROS, it's strongly recommended to <a href="http://wiki.ros.org/rosbag/Commandline">learn its usage</a>. <a class="footnote-backref" href="#fnref:rosbag" rev="footnote" title="Jump back to footnote 6 in the text">↩</a></p>
</li>
<li id="fn:gtsam_examples">
<p>GTSAM's <em>examples</em> folder is also a very good source to learn about SLAM in general. <a class="footnote-backref" href="#fnref:gtsam_examples" rev="footnote" title="Jump back to footnote 7 in the text">↩</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
  <div class="article_meta">
    <p>Posted on: sab 30 luglio 2016</p>
    <p>Category: <a href="http://nicolovaligi.com/category/3_camera.html">3_camera</a>
 &ndash; Tags:
      <a href="http://nicolovaligi.com/tag/robotics.html">robotics</a>,      <a href="http://nicolovaligi.com/tag/perception.html">perception</a>    </p>
  </div>


</article>


    <div id="ending_message">
      <p>&copy; Nicolò Valigi. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme originally by Giulio Fidente on <a href="https://github.com/gfidente/pelican-svbhack" target="_blank">github</a>. </p>
    </div>
  </main>
</body>
</html>