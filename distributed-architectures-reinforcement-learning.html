<!DOCTYPE html>
<html lang="en">

<head>
  <!-- ## for client-side less
  <link rel="stylesheet/less" type="text/css" href="//nicolovaligi.com/theme/css/style.less">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/less.js/1.7.3/less.min.js" type="text/javascript"></script>
  -->
  <link rel="stylesheet" type="text/css" href="//nicolovaligi.com/theme/css/style.css">
  <link rel="stylesheet" type="text/css" href="//nicolovaligi.com/theme/css/pygments.css">
  <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=PT+Sans|PT+Serif|PT+Mono">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Nicolò Valigi">
  <meta name="description" content="Posts and writings by Nicolò Valigi">

  <link href="https://nicolovaligi.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Nicolò Valigi Atom" />

<meta name="keywords" content="reinforcement-learning">

  <title>
History of distributed architectures for Reinforcement Learning -
    Nicolò Valigi
  </title>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-67820015-1', 'auto');
  ga('send', 'pageview');

</script></head>

<body>
  <aside>
    <div id="user_meta">
      <a href="//nicolovaligi.com">
        <img src="/blog/images/logo_white.jpg" alt="logo">
      </a>
      <h2><a href="//nicolovaligi.com">Nicolò Valigi</a></h2>
      <p>Writing about Software, Robots, and Machine Learning.</p>
      <ul class="links">
        <li><a href="/pages/talks.html">Talks</a></li>
        <li><a href="/pages/robotics-for-developers-tutorial.html">Robotics for developers</a></li>
        <li><a href="/pages/research.html">Research</a></li>
        <li><a href="/feeds/all.atom.xml">RSS feed</a></li>
      </ul>

      <ul style="padding-left: 0;
                 margin-left: -5px;
                 list-style: none;
                 text-align: center;">

        <!-- github -->
        <li>
            <a href="https://github.com/nicolov">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

        <!-- mail -->
        <li>
            <a href="mailto:nicolo.valigi@gmail.com">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

        <!-- RSS -->
        <li>
            <a href="/feeds/all.atom.xml">
                <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                </span>
            </a>
        </li>

    </ul>

    </div>
  </aside>

  <main>
    <header>
      <p>
      <a href="//nicolovaligi.com">Index</a> &nbsp; &brvbar; &nbsp;
      <a href="//nicolovaligi.com/tags.html">Tags</a> &nbsp; &brvbar; &nbsp;
      <a href="//nicolovaligi.com/archives.html">Archives</a>
      &brvbar; <a href="https://nicolovaligi.com/feeds/all.atom.xml">Atom</a>
      </p>
    </header>

<article>
  <div class="article_title">
    <h1><a href="//nicolovaligi.com/distributed-architectures-reinforcement-learning.html">History of distributed architectures for Reinforcement Learning</a></h1>
  </div>
  <div class="article_text">
    <p>Many recent successes of Reinforcement Learning are fueled by a massive increase
in the scale of experiments. This article traces the evolution of the
distributed architectures used to support these models and discusses how
algorithm families (eg on-policy, off-policy) affect implementation choices at
datacenter scale.</p>
<h2>In principle, there was Supervised Learning</h2>
<p>In Supervised Learning, the training data distribution is stationary.  Gradient
Descent in SL is an (almost) "embarassingly parallel" problem in which multiple
workers can progress independently with few communication bottlenecks. <em>Data
parallelism</em>, where different workers train in parallel on different chunks of
the training dataset is a great fit for Stochastic Gradient Descent.</p>
<p>The archetype for  this approach is <strong>DistBelief</strong>. <a href='#distbelief' id='ref-distbelief-1'>Dean et al. (2012)</a> Multiple
workers sample batches of training data, compute gradients, and send them to a
global parameter server which applies them to the shared instance of the model.
Since the training data distribution is stationary, the parameter server can
simply average incoming gradients before applying them (ie the gradient and sum
operators commute).  Individual workers can also take advantage of <em>model
parallelism</em> and break the model down in multiple parts to fit in limited GPU
memory. The parameter server itself can also be sharded to cope with the traffic
generated by many workers.</p>
<p>Reinforcement Learning doesn't enjoy the same luxury. For one, models need to go
out and fetch their own data from the environment. Even more worringly, the
training data itself depends on the policy, and therefore can't be reused
willy-nilly (this is true even in off-policy algorithms). As a result, each RL
breakthrough relies on a delicate interplay of algorithms, distributed
architectures and plain hacks.</p>
<p>In the next sections, we'll follow the history of distributed architectures for
the two main families of RL algorithms: state-based and policy-based. We'll see
how they recycle and iterate on the same ideas and have somewhat converged over
the past few years. Basic knowledge of RL fundamentals and vocabulary is
assumed.</p>
<h2>Architectures for Q-Learning algorithms</h2>
<p>Q-Learning algorithms are value-based off-policy algorithms especially suited
for discrete action spaces.</p>
<p>Google's 2015 paper about <strong>Gorila</strong> describes the archetypical architecture for
distributed Q-Learning at scale. <a href='#gorila' id='ref-gorila-1'>Nair et al. (2015)</a> <em>Gorila</em> follows the same blueprint
as DeepBelief, with independent agents that collect samples from the
environment, update their parameters, and communicate gradients back to a global
parameter server. In the "bundled" configuration analyzed in the paper, each
worker also has its dedicated <em>replay memory</em> to store state transitions and
periodically downloads an updated copy of the Q model from the parameter server.
As sketched out in the paper, workers are designed to run on different machines
within a datacenter, and coordinate over the network through the parameter
server.</p>
<p><img class="img-center" src="//nicolovaligi.com/graphics-1.svg" style="max-width: 80%; transform: scale(1);"/></p>
<p>DeepMind's <strong>Ape-X</strong> architecture from 2018 is another approach for distributed
Q-Learning where the focus is not on parallelizing learning (as in <em>Gorila</em>),
but data collection. <a href='#apex' id='ref-apex-1'>Horgan et al. (2018)</a> Again, we have independent workers that periodically sync
with a global copy of the Q-Network. Compared to <em>Gorila</em>, however, workers
share experiences rather than gradients. This is achieved through a <em>global</em>
replay buffer which receives experience samples from all independent workers.</p>
<p><img class="img-center" src="//nicolovaligi.com/graphics-2.svg" style="max-width: 80%; transform: scale(1);"/></p>
<p>Since Ape-X takes advantage of <em>Prioritized Experience Replay</em>, a single GPU
worker has enough capacity to keep up with the experiences produced by hundreds
of CPU workers. The insight behind sharing experiences instead of gradients is
that they remain useful for longer because they're not as strongly dependent on
the current Q function.</p>
<h2>Architectures for Actor-Critic algorithms</h2>
<p>Actor-critic algorithms are a family of on-policy algorithms that combine a
policy-based (<em>actor</em>) and value-based (<em>critic</em>) models with the goal of
improving the performance of vanilla policy gradients (mainly by reducing variance).</p>
<p>The landmark architecture for Actor-Critic methods is <strong>A3C</strong>, which came out in
2016. <a href='#a3c' id='ref-a3c-1'>Mnih et al. (2016)</a> A3C also uses asynchronous actor-learners, but implements them as
CPU threads within a single physical host, therefore eliminating network
communication overhead. Each worker maintains a local copy of the value and
policy networks, and periodically synchronizes them with the global copy.</p>
<p><img class="img-center" src="//nicolovaligi.com/graphics-3.svg" style="max-width: 80%; transform: scale(1);"/></p>
<p>The algorithmic insight behind A3C is to take advantage of parallelism to reduce
the adverse effects of correlation in the training data. In other words,
aggregating values from multiple concurrent actors effectively achieves a
similar result as random sampling from the replay buffer in Q-Learning.</p>
<p>A3C doesn't scale well beyond 16 concurrent actor threads because the
local policy gradients quickly become outdated as the updates from other threads
are included asynchronously in the global model. On-policy algorithms like
Actor-Critic don't cope well with this <em>policy lag</em> problem.</p>
<p>Pratically, it turns out that A3C is little more than a thought exercise. A
non-distributed version of A3C, <strong>A2C</strong>, obtains the same performance with
simpler code. <a href='#a2c' id='ref-a2c-1'>Stooke and Abbeel (2018)</a> A2C achieves a higher throughput than A3C by (synchronously)
batching together updates from multiple environments and running gradient
descent on a GPU. In effect, the regularization and exploration benefits in A3C
should not be attributed to async-induced noise, but to the presence of multiple
independent exploration policies.</p>
<p>DeepMind's <strong>IMPALA</strong> from 2018 takes A2C further and converges towards the
multiple-actor, single-learner model proposed in Ape-X. <a href='#impala' id='ref-impala-1'>Espeholt et al. (2018)</a> In contrast to
A3C, IMPALA workers communicate experiences, and not gradients, just like Ape-X.
However, Ape-X runs on-policy algorithms which can learn from any state
transition, whereas IMPALA is an Actor-Critic that can only learn from on-policy
experience.  In other words, experience collected by workers under out-of-date
policies corrupts the policy gradient (<em>policy lag</em>). IMPALA uses a novel
importance sampling algorithm (<em>V-trace</em>) to work around this problem without
throwing away samples.</p>
<p><img class="img-center" src="//nicolovaligi.com/graphics-4.svg" style="max-width: 80%; transform: scale(1);"/></p>
<p>Another success story of large-scale RL based on Actor-Critic methods was
reported by OpenAI when their <strong>OpenAI Five</strong> agent defeated the Dota 2 world
champions. <a href='#openai2019dota' id='ref-openai2019dota-1'>OpenAI et al. (2019)</a> Like IMPALA, actors submit samples to
a pool of GPU optimizers, each of which maintains a local experience buffer and
samples it to produce policy gradients for the PPO algorithm. Gradients are
aggregated across the GPU pool and applied synchronously.</p>
<p>Both the Dota game engine and the OpenAI Five model are significantly more
complex than the Atari games used in other experiments. Under these conditions,
it's more efficient to create a dedicated pool of GPU machines to run the
forward pass on the latest policy, rather than leaving this responsibility in
the actors. Coupled with the synchronous policy updates, this likely explains
why the paper doesn't make any mention of policy-lag corrections.</p>
<h2>Takeaways</h2>
<p>From the architecture point of view, the main difference between state-based and
policy-based algorithms is that the former are generally off-policy, whereas the
latter usually need on-policy samples. Nevertheless, the most recent designs for
both families have converged on a similar design:</p>
<ul>
<li>
<p>Both Ape-X and IMPALA share experiences instead of gradients to reduce
  communication overhead. Q-Learning adapts naturally to this change, but
  Actor-Critics need correction factors.</p>
</li>
<li>
<p>Likewise, distributed learning has fallen out of favor given the rapid
  increase in GPU capacity. In recent designs, each GPU learner can service
  hundreds of actor CPUs and reduce communication overhead.</p>
</li>
</ul>
<p>If we take for granted that interactions with the environment (ie actors) need
to be parallelized (that's the whole point of this exercise in cloud madness),
the real choice is whether learning should be as well. The recent consensus
seems to be <em>no, GPUs are enough for learning</em>.</p>
<p>Probably the most enduring lesson from this review is to be careful about
recognizing where accidental complexity and implementation details begin to
obscure novel ideas. Case in point is A3C's supposedly better exploration
behavior due to asynchrony, which later turned out to be inferior to synchronous
methods in both performance and cost.</p><hr>
<h2>Bibliography</h2>
<p id='distbelief'>Jeffrey Dean, Greg&nbsp;S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc&nbsp;V. Le, Mark&nbsp;Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke&nbsp;Yang, and Andrew&nbsp;Y. Ng.
Large scale distributed deep networks.
In <em>NIPS</em>. 2012. <a class="cite-backref" href="#ref-distbelief-1" title="Jump back to reference 1">↩</a></p>
<p id='impala'>Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu.
Impala: scalable distributed deep-rl with importance weighted actor-learner architectures.
2018.
<a href="http://arxiv.org/abs/1802.01561">arXiv:1802.01561</a>. <a class="cite-backref" href="#ref-impala-1" title="Jump back to reference 1">↩</a></p>
<p id='apex'>Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and David Silver.
Distributed prioritized experience replay.
2018.
<a href="http://arxiv.org/abs/1803.00933">arXiv:1803.00933</a>. <a class="cite-backref" href="#ref-apex-1" title="Jump back to reference 1">↩</a></p>
<p id='a3c'>Volodymyr Mnih, Adrià&nbsp;Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy&nbsp;P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
Asynchronous methods for deep reinforcement learning.
2016.
<a href="http://arxiv.org/abs/1602.01783">arXiv:1602.01783</a>. <a class="cite-backref" href="#ref-a3c-1" title="Jump back to reference 1">↩</a></p>
<p id='gorila'>Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro&nbsp;De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, and David Silver.
Massively parallel methods for deep reinforcement learning.
2015.
<a href="http://arxiv.org/abs/1507.04296">arXiv:1507.04296</a>. <a class="cite-backref" href="#ref-gorila-1" title="Jump back to reference 1">↩</a></p>
<p id='openai2019dota'>OpenAI, :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique&nbsp;Pondé de&nbsp;Oliveira&nbsp;Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang.
Dota 2 with large scale deep reinforcement learning.
2019.
<a href="http://arxiv.org/abs/1912.06680">arXiv:1912.06680</a>. <a class="cite-backref" href="#ref-openai2019dota-1" title="Jump back to reference 1">↩</a></p>
<p id='a2c'>Adam Stooke and Pieter Abbeel.
Accelerated methods for deep reinforcement learning.
2018.
<a href="http://arxiv.org/abs/1803.02811">arXiv:1803.02811</a>. <a class="cite-backref" href="#ref-a2c-1" title="Jump back to reference 1">↩</a></p>

  </div>
  <div class="article_meta">
    <p>Posted on: gio 20 febbraio 2020</p>
    <p>Category: <a href="//nicolovaligi.com/category/2020-02-20-distributed-rl-architectures.html">2020-02-20-distributed-rl-architectures</a>
 &ndash; Tags:
      <a href="//nicolovaligi.com/tag/reinforcement-learning.html">reinforcement-learning</a>    </p>
  </div>


</article>


    <div id="ending_message">
      <p>&copy; Nicolò Valigi. Built using <a href="http://getpelican.com" target="_blank">Pelican</a>. Theme originally by Giulio Fidente on <a href="https://github.com/gfidente/pelican-svbhack" target="_blank">github</a>. </p>
    </div>
  </main>
</body>
</html>